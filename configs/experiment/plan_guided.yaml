# @package _global_

global_cfg: # would be passed to actor, critic1, critic2, policy, env
  null

runner:
  _target_: src.runner.PlanGuidedRunner
  _partial_: true

diffuser:
  dir: /output/hydra_log/RL_Diffuser/runs/2023-09-03_07-10-56_216947/
  epoch: last

guide:
  _target_: diffuser.sampling.NoTrainGuideYLower

policy:
  _target_: diffuser.sampling.GuidedPolicy
  _partial_: true
  # guide: in python
  # diffusion_model: in python
  # normalizer:  in python
  preprocess_fns: []
  # the following are **sample_kwargs
  sample_fn: 
    _target_: diffuser.sampling.n_step_guided_p_sample
    _partial_: true
  scale: 1000
  n_guide_steps: 2
  t_stopgrad: 2 # positive: grad[t < t_stopgrad] = 0; bigger is noise
  scale_grad_by_std: true

trainer: # for plan guided
  max_episode_length: 1000
  batch_size: 10 # make 4 plan parallelly and then train
  verbose: false # the diffusion step progress bar
  use_controller_act: true # use basic controller to cal action instead of the results of diffuser
  plan_once: true # only plan once for each episode, the following action are from controller
  custom_target: null # if not null, use the custom goal instead of the goal from env


vis_freq: 50
max_render: 1


# common - for all tasks (task_name, tags, output_dir, device)
algorithm_name: "DefaultAlgName"
task_name: "RL_Diffuser"
tags: ["debug"]